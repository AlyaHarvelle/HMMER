{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:55:55.284130Z",
     "start_time": "2023-10-02T07:55:53.625394Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import the libraries and functions\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "We start with uploading the file containing information regarding the disordered regions (`curated.mjson`) which further will be used to filter the instances from the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum width of the columns\n",
    "pd.set_option('display.max_colwidth', 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:55:59.517844Z",
     "start_time": "2023-10-02T07:55:59.424142Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of instances in the Curated Disprot database: 3151\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>evidence</th>\n",
       "      <th>feature</th>\n",
       "      <th>source</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>length</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6927</th>\n",
       "      <td>P03265</td>\n",
       "      <td>curated</td>\n",
       "      <td>disorder</td>\n",
       "      <td>disprot</td>\n",
       "      <td>294</td>\n",
       "      <td>334</td>\n",
       "      <td>41</td>\n",
       "      <td>P03265_294-334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6928</th>\n",
       "      <td>P03265</td>\n",
       "      <td>curated</td>\n",
       "      <td>disorder</td>\n",
       "      <td>disprot</td>\n",
       "      <td>454</td>\n",
       "      <td>464</td>\n",
       "      <td>11</td>\n",
       "      <td>P03265_454-464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6929</th>\n",
       "      <td>P49913</td>\n",
       "      <td>curated</td>\n",
       "      <td>disorder</td>\n",
       "      <td>disprot</td>\n",
       "      <td>134</td>\n",
       "      <td>170</td>\n",
       "      <td>37</td>\n",
       "      <td>P49913_134-170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6930</th>\n",
       "      <td>P03045</td>\n",
       "      <td>curated</td>\n",
       "      <td>disorder</td>\n",
       "      <td>disprot</td>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "      <td>107</td>\n",
       "      <td>P03045_1-107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6931</th>\n",
       "      <td>P00004</td>\n",
       "      <td>curated</td>\n",
       "      <td>disorder</td>\n",
       "      <td>disprot</td>\n",
       "      <td>1</td>\n",
       "      <td>105</td>\n",
       "      <td>105</td>\n",
       "      <td>P00004_1-105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         acc evidence   feature   source  start  end  length          region\n",
       "6927  P03265  curated  disorder  disprot    294  334      41  P03265_294-334\n",
       "6928  P03265  curated  disorder  disprot    454  464      11  P03265_454-464\n",
       "6929  P49913  curated  disorder  disprot    134  170      37  P49913_134-170\n",
       "6930  P03045  curated  disorder  disprot      1  107     107    P03045_1-107\n",
       "6931  P00004  curated  disorder  disprot      1  105     105    P00004_1-105"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe with DisProt instances from curated.mjson database\n",
    "data = list()\n",
    "\n",
    "with open('curated.mjson', 'r') as file:\n",
    "    for line in file:\n",
    "        obj = json.loads(line)\n",
    "        rows = json_parser(obj)\n",
    "        data.extend(rows)\n",
    "\n",
    "curated_disprot = pd.DataFrame(data)\n",
    "\n",
    "# Calculate the length of disordered regions\n",
    "curated_disprot['length'] = curated_disprot['end'] - curated_disprot['start'] + 1\n",
    "curated_disprot = curated_disprot[(curated_disprot['feature'] == 'disorder') & (curated_disprot['source'] == 'disprot')]\n",
    "curated_disprot['region'] = curated_disprot.apply(lambda row: f\"{row['acc']}_{row['start']}-{row['end']}\", axis=1)\n",
    "curated_disprot.to_csv('curated_disprot.csv', index=False)\n",
    "\n",
    "print(f'The number of instances in the Curated Disprot database: {len(curated_disprot)}')\n",
    "curated_disprot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from separate columns with the Uniprot ID, database related information and the start-end position of the disordered region we create a `region` column comprising the ID, start and end position. This will be used for further disordered regions selection.\n",
    "\n",
    "Then we upload the XML file with the BLAST results and filter it using the previously loaded `curated_disprot` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the path to the input XML files\n",
    "database_path = '{}/databases/uniprot'.format(directory)\n",
    "files = os.listdir(database_path)\n",
    "\n",
    "common_df = pd.DataFrame()\n",
    "\n",
    "# Merge files into one dataframe\n",
    "for file_name in files:\n",
    "    file_path = os.path.join(database_path, file_name)\n",
    "    df = blast_parser(file_path) # iterates over XML files\n",
    "    common_df = pd.concat([common_df, df], ignore_index=True)\n",
    "\n",
    "# print(f'The number of instances in the dataframe: {len(common_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows with the disordered regions: 7393\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>subject_id</th>\n",
       "      <th>query_len</th>\n",
       "      <th>hsp_len</th>\n",
       "      <th>query_seq</th>\n",
       "      <th>match_seq</th>\n",
       "      <th>subject_seq</th>\n",
       "      <th>query_start</th>\n",
       "      <th>query_end</th>\n",
       "      <th>subject_start</th>\n",
       "      <th>subject_end</th>\n",
       "      <th>identity</th>\n",
       "      <th>positive</th>\n",
       "      <th>gaps</th>\n",
       "      <th>eval</th>\n",
       "      <th>bit_score</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Q9H832</td>\n",
       "      <td>A0A6J2FM24</td>\n",
       "      <td>354</td>\n",
       "      <td>356</td>\n",
       "      <td>MAESPTEEAATA--GA...</td>\n",
       "      <td>MAESPTEEAATA  GA...</td>\n",
       "      <td>MAESPTEEAATATAGA...</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1851.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Q9H832</td>\n",
       "      <td>A0A3Q7W6Y2</td>\n",
       "      <td>354</td>\n",
       "      <td>356</td>\n",
       "      <td>MAESPTEEAATA--GA...</td>\n",
       "      <td>MAESPTEEAATA  GA...</td>\n",
       "      <td>MAESPTEEAATATAGA...</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1851.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Q9H832</td>\n",
       "      <td>A0A2U3VK69</td>\n",
       "      <td>354</td>\n",
       "      <td>356</td>\n",
       "      <td>MAESPTEEAATA--GA...</td>\n",
       "      <td>MAESPTEEAATA  GA...</td>\n",
       "      <td>MAESPTEEAATATAGA...</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1851.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Q9H832</td>\n",
       "      <td>A0A2Y9JVH5</td>\n",
       "      <td>354</td>\n",
       "      <td>358</td>\n",
       "      <td>MAESPTEEAATA----...</td>\n",
       "      <td>MAESPTEEAATA    ...</td>\n",
       "      <td>MAESPTEEAATATATA...</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>1</td>\n",
       "      <td>356</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1854.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>Q9H832</td>\n",
       "      <td>A0A8C7ALE4</td>\n",
       "      <td>354</td>\n",
       "      <td>358</td>\n",
       "      <td>MAESPTEEAATA----...</td>\n",
       "      <td>MAESPTEEAATA    ...</td>\n",
       "      <td>MAESPTEEAATATATA...</td>\n",
       "      <td>1</td>\n",
       "      <td>354</td>\n",
       "      <td>1</td>\n",
       "      <td>356</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1854.0</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    query_id  subject_id  query_len  hsp_len            query_seq  \\\n",
       "200   Q9H832  A0A6J2FM24        354      356  MAESPTEEAATA--GA...   \n",
       "201   Q9H832  A0A3Q7W6Y2        354      356  MAESPTEEAATA--GA...   \n",
       "202   Q9H832  A0A2U3VK69        354      356  MAESPTEEAATA--GA...   \n",
       "203   Q9H832  A0A2Y9JVH5        354      358  MAESPTEEAATA----...   \n",
       "204   Q9H832  A0A8C7ALE4        354      358  MAESPTEEAATA----...   \n",
       "\n",
       "               match_seq          subject_seq  query_start  query_end  \\\n",
       "200  MAESPTEEAATA  GA...  MAESPTEEAATATAGA...            1        354   \n",
       "201  MAESPTEEAATA  GA...  MAESPTEEAATATAGA...            1        354   \n",
       "202  MAESPTEEAATA  GA...  MAESPTEEAATATAGA...            1        354   \n",
       "203  MAESPTEEAATA    ...  MAESPTEEAATATATA...            1        354   \n",
       "204  MAESPTEEAATA    ...  MAESPTEEAATATATA...            1        354   \n",
       "\n",
       "     subject_start  subject_end  identity  positive  gaps  eval  bit_score  \\\n",
       "200              1          354       350       350     4   0.0     1851.0   \n",
       "201              1          354       350       350     4   0.0     1851.0   \n",
       "202              1          354       350       350     4   0.0     1851.0   \n",
       "203              1          356       351       351     6   0.0     1854.0   \n",
       "204              1          356       351       351     6   0.0     1854.0   \n",
       "\n",
       "     count  \n",
       "200    200  \n",
       "201    200  \n",
       "202    200  \n",
       "203    200  \n",
       "204    200  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter only disordered regions in the common_df filtering with the Uniprot IDs from the curated_disordered\n",
    "disordered = common_df[common_df['query_id'].isin(curated_disprot['acc'])]\n",
    "disordered.to_csv('disordered_df.csv', index=False)\n",
    "\n",
    "print(f'The number of rows with the disordered regions: {len(disordered)}')\n",
    "disordered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:55:58.503052Z",
     "start_time": "2023-10-02T07:55:57.774314Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Open XML file as a dataframe - for one file\n",
    "# input_file = '{}/databases/uniprot/curated_uniprot.fasta_75'.format(directory) # change the file name if necessary\n",
    "# df = blast_parser(input_file)\n",
    "# print(f'The number of instances: {len(df)}')\n",
    "\n",
    "# # Keep only disordered regions in the initial dataframe filtering with curated_disordered dataframe\n",
    "# disordered = df[df['query_id'].isin(curated_disprot['acc'])]\n",
    "# disordered.to_csv('disordered_df.csv', index=False)\n",
    "\n",
    "# print(f'The number of rows with the disordered regions: {len(disordered)}')\n",
    "# disordered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Extract the disordered regions IDs and positions\n",
    "# dis_regs = set()\n",
    "\n",
    "# for i, row in disordered.iterrows():\n",
    "#     dis_id = row[0]\n",
    "#     matching_row = curated_disprot[curated_disprot['acc'] == dis_id]\n",
    "#     if not matching_row.empty:\n",
    "#         region = matching_row['region']\n",
    "#         dis_regs.update(region)\n",
    "\n",
    "# print(f'The number of disordered regions in the database: {len(dis_regs)}')\n",
    "\n",
    "# # Define an array of disordered regions ids\n",
    "# disprot_ids = dis_regs\n",
    "\n",
    "# # Dropdown list of Uniprot query IDs for disordered regions\n",
    "# output = widgets.Select(options=disprot_ids,\n",
    "#     rows=10,\n",
    "#     description='Uniprot ID: ',\n",
    "#     layout={'width': 'max-content'},\n",
    "#     disabled=False)\n",
    "\n",
    "# display(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:56:04.330580Z",
     "start_time": "2023-10-02T07:56:04.268997Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save the data for building MSA of the disordered regions\n",
    "# id_dis = output.value # the Uniprot ID + start and end positions\n",
    "# id_split = id_dis.split('_')[0] # the Uniprot ID\n",
    "# i = 1 # change to the necessary region\n",
    "# store the same id for all notebooks\n",
    "# %store id_dis \n",
    "# id_split\n",
    "# %store i\n",
    "# selected_dis = disordered[disordered['query_id'].isin([id_dis.split('_')[0]])] # the information for one query ID\n",
    "# print(f'The number of sequences for the MSA of the {id_split} protein: {len(selected_dis)}')\n",
    "# selected_dis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the curated_disprot dataframe based on selected ID\n",
    "# curated_query = curated_disprot[curated_disprot['region'] == id_dis]\n",
    "# curated_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multiple Sequence Alignment\n",
    "\n",
    "Within this framework, we will compare the MSA results obtained directly from the BLAST output with those generated from the ClustalOmega and MAFFT.\n",
    "\n",
    "### 1.1 MSA from the BLAST output (local alignment)\n",
    "This code iterates through each row of the dataframe for a previously selected query ID. If there is no gap, it maps the amino acid from the subject sequence to the corresponding position in the query sequence, starting from the beginning of the query sequence. The resulting mapped amino acids are then added to a new count. In the end we have a FASTA file with the subject IDs and the aligned sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Retrieve query sequence and its length from the disordered dataframe\n",
    "# query_sequence = disordered[disordered['query_id'] == id_split]['query_seq'].unique()[0]\n",
    "# query_sequence = re.sub(r'[-]', '', query_sequence)\n",
    "# query_len = disordered[disordered['query_id'] == id_split]['query_len'].unique()[0]\n",
    "# print(f'The Uniprot ID: {id_split}', '\\n'\n",
    "#       f'The length of the sequence: {query_len}', '\\n'\n",
    "#       f'The sequence: {query_sequence}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-02T07:57:48.083609Z",
     "start_time": "2023-10-02T07:56:11.171489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of proteins with the disordered regions: 39\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>disprot_id</th>\n",
       "      <th>query_sequence</th>\n",
       "      <th>query_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q9H832</td>\n",
       "      <td>MAESPTEEAATAGAGA...</td>\n",
       "      <td>354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q8IW19</td>\n",
       "      <td>MSGGFELQPRDGGPRV...</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q99967</td>\n",
       "      <td>MADHMMAMNHGRFPDG...</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q9CXY6</td>\n",
       "      <td>MRGDRGRGRGGRFGSR...</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q8R464</td>\n",
       "      <td>PLLLLWAAAAGPGTGQ...</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  disprot_id       query_sequence  query_len\n",
       "0     Q9H832  MAESPTEEAATAGAGA...        354\n",
       "1     Q8IW19  MSGGFELQPRDGGPRV...        511\n",
       "2     Q99967  MADHMMAMNHGRFPDG...        270\n",
       "3     Q9CXY6  MRGDRGRGRGGRFGSR...        390\n",
       "4     Q8R464  PLLLLWAAAAGPGTGQ...        379"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe of disprot sequences\n",
    "uniprot_ids = disordered['query_id'].unique()\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in uniprot_ids:\n",
    "    query_sequence = disordered[disordered['query_id'] == i]['query_seq'].unique()[0]\n",
    "    query_sequence = re.sub(r'[-]', '', query_sequence)\n",
    "    query_len = len(query_sequence)\n",
    "    \n",
    "    data.append([i, query_sequence, query_len])\n",
    "\n",
    "disprot_sequences = pd.DataFrame(data, columns=['disprot_id', 'query_sequence', 'query_len'])\n",
    "print(f'The number of proteins with the disordered regions: {len(disprot_sequences)}')\n",
    "disprot_sequences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 1.2 Build the MSA from the BLAST - for all alignments\n",
    "# # Iterate through each row in the disprot_sequences \n",
    "# for ind_q, row_q in disprot_sequences.iterrows():\n",
    "#     query_id = row_q['disprot_id']\n",
    "#     query_sequence = row_q['query_sequence']\n",
    "#     query_len = row_q['query_len']\n",
    "#     selected_dis = disordered[disordered['query_id'].isin([query_id])]\n",
    "#     print(ind_q, query_id, query_len, query_sequence[:3],\"...\", query_sequence[-3:])\n",
    "    \n",
    "#     # Save the file\n",
    "#     out_file = f'{directory}/results/alignments/output_files/blast/{query_id}_blast.fasta'\n",
    "    \n",
    "#     # Initialize the first row with a length of the query sequence\n",
    "#     with open(out_file, 'w') as fout:\n",
    "#         mapped_seq = ['-'] * query_len\n",
    "\n",
    "#         # Write the header line for the query sequence\n",
    "#         fout.write('>{}\\n'.format(query_id))\n",
    "\n",
    "#         # Map the query sequence to the mapped_seq list\n",
    "#         c = 0\n",
    "#         for l_q in query_sequence:\n",
    "#             if l_q != ' ' and l_q != '-':\n",
    "#                 mapped_seq[c] = l_q\n",
    "#                 c += 1\n",
    "\n",
    "#         # Write the query_mapped_seq sequence to the output file\n",
    "#         fout.write('{}\\n'.format(''.join(mapped_seq)))\n",
    "\n",
    "#         # Map the subject sequences to the mapped_seq list and write to the output file\n",
    "#         for index, row in selected_dis.iterrows():\n",
    "#             if query_id == row['subject_id']:\n",
    "#                 continue\n",
    "                \n",
    "#             c = 0\n",
    "#             query_start = row['query_start']\n",
    "#             for l_q, l_s in zip(row['query_seq'], row['subject_seq']):\n",
    "#                 if l_q != ' ' and l_q != '-': # if the initial aa from query is not empty or gapped\n",
    "#                     if query_start + c - 1 < len(mapped_seq): # added the condition\n",
    "#                         mapped_seq[query_start + c - 1] = l_s if l_s != ' ' else '-' # assign aa to subject\n",
    "#                         c += 1\n",
    "#             fout.write('>{}\\n{}\\n'.format(row['subject_id'], ''.join(mapped_seq)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. MSA from the ClustalOmega (global alignment)\n",
    "\n",
    "Initially, we need to preprocess the dataframe into a suitable input format file for ClustalOmega. For each Uniprot ID (query and all the subject) we extract the sequence from Uniprot using `get_fasta` function. \n",
    "\n",
    "These sequences are NOT aligned at that time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUNS FOR A LONG TIME\n",
    "# # Retrieve the unaligned sequences from the local machine - for all disprot sequences at once\n",
    "# for d_id, d_row in disprot_sequences.iterrows():\n",
    "#     id_split = d_row['disprot_id']\n",
    "#     seq = d_row['query_sequence']\n",
    "#     output_file = f'{directory}/results/alignments/input_files/{id_split}_input.fasta'\n",
    "    \n",
    "#     # Iterating over selected_dis\n",
    "#     with open(output_file, 'w') as fout:\n",
    "#         selected_dis = disordered[disordered['query_id'].isin([id_split])]\n",
    "#         # Write the query sequence to the output file as the first line\n",
    "#         fout.write(\">{}\\n{}\\n\".format(id_split, seq))\n",
    "#         print(id_split, len(seq)) # correct\n",
    "\n",
    "#         for index, row in selected_dis.iterrows():\n",
    "#             accession = row['subject_id']\n",
    "#             sequence = get_fasta(accession)\n",
    "#             print(index, accession, len(sequence))\n",
    "#             if id_split == accession: # remove duplicates\n",
    "#                 continue\n",
    "#             fout.write(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T09:29:46.863906Z",
     "start_time": "2023-09-14T09:29:46.827846Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Retrieve the unaligned sequences from the local machine - for one sequence\n",
    "# output_file = f'{directory}/results/alignments/input_files/{id_split}_input.fasta'\n",
    "\n",
    "# with open(output_file, 'w') as fout:\n",
    "#     # Write the query sequence to the output file as the first line\n",
    "#     fout.write('>{}\\n{}\\n'.format(id_split, query_sequence))\n",
    "\n",
    "#     for index, row in selected_dis.iterrows():\n",
    "#         accession = row['subject_id']\n",
    "#         sequence = get_fasta(accession)\n",
    "#         print(index, accession, len(sequence))\n",
    "#         if id_split == accession: # remove duplicates\n",
    "#             continue\n",
    "#         fout.write(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClustalOmega MSA generator\n",
    "def clustalo_generator(input_folder, output_folder):\n",
    "    # Iterate over all input files in the input folder\n",
    "    for input_file in os.listdir(input_folder):\n",
    "        if input_file.endswith('.fasta'): # remove 1 later\n",
    "            # Extract ID from the file name\n",
    "            id_split = os.path.splitext(input_file)[0].split('_')[0]\n",
    "            print(f'MSA ClustalOmega is generated for the {id_split} protein')\n",
    "            output_file = os.path.join(output_folder, f'{id_split}_clustal.fasta') # name of the output file\n",
    "\n",
    "            # Define the ClustalOmega command\n",
    "            clustalomega_cline = ClustalOmegaCommandline(\n",
    "                infile=os.path.join(input_folder, input_file),\n",
    "                outfile=output_file,\n",
    "                outputorder='input-order',\n",
    "                verbose=False,\n",
    "                auto=True,\n",
    "                force=True)\n",
    "\n",
    "            # Run the ClustalOmega command\n",
    "            subprocess.run(str(clustalomega_cline), shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After obtaining the sequences, we build the ClustalOmega MSA using the function `clustalo_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P00742\n",
      "Q93KQ4\n",
      "Q9BYF1\n",
      "A4L7I2\n",
      "O14958\n",
      "P07342\n",
      "A8AZZ3\n",
      "O14727\n",
      "P01019\n",
      "Q9CXY6\n",
      "Q9H832\n",
      "Q86FP8\n",
      "P01097\n",
      "O15922\n",
      "Q9BYI3\n",
      "S7W634\n",
      "J8TM36\n",
      "O00585\n",
      "A1L1Q4\n",
      "P00392\n",
      "Q8IU57\n",
      "O43474\n",
      "Q84852\n",
      "P00736\n",
      "Q8IW19\n",
      "Q8R464\n",
      "Q99967\n",
      "Q8WUG5\n",
      "Q5VZK9\n",
      "O35274\n",
      "O00308\n",
      "S6B291\n",
      "Q9H0E2\n",
      "O43791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BisectingKmeans(): Can't split cluster no. 0 which has 104 objects any further. Hope it's not too big and doesn't slow things down.\n",
      "WARNING: BisectingKmeans(): Can't split cluster no. 1 which has 104 objects any further. Hope it's not too big and doesn't slow things down.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q5T4W7\n",
      "O88339\n",
      "P04370-5\n",
      "Q9Z2F5\n",
      "Q8K4J6\n"
     ]
    }
   ],
   "source": [
    "# Run the ClustalOmega generator - it takes a while\n",
    "input_folder = directory + '/results/alignments/input_files'\n",
    "output_folder = directory + '/results/alignments/output_files/clustal'\n",
    "\n",
    "clustalo_generator(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove gaps from the query sequence of the alignment\n",
    "def remove_extra_gaps(alignment):\n",
    "    # Get the first sequence\n",
    "    first_seq = alignment[0].seq\n",
    "\n",
    "    # Find the positions of non-gap characters in the first sequence\n",
    "    non_gap_positions = [i for i, base in enumerate(first_seq) if base != '-']\n",
    "\n",
    "    # Create a list to hold SeqRecord objects for the filtered alignment\n",
    "    filtered_seqs = []\n",
    "\n",
    "    for seq_record in alignment:\n",
    "        # Extract and join the non-gap characters\n",
    "        filtered_seq = Seq(''.join(seq_record.seq[i] for i in non_gap_positions), seq_record.seq.alphabet)\n",
    "        \n",
    "        # Create a new SeqRecord with the filtered sequence\n",
    "        filtered_record = SeqRecord(\n",
    "            seq=filtered_seq,\n",
    "            id=seq_record.id,\n",
    "            description=seq_record.description)\n",
    "        \n",
    "        filtered_seqs.append(filtered_record)\n",
    "\n",
    "    # Create a new alignment with filtered SeqRecord objects\n",
    "    new_alignment = MultipleSeqAlignment(filtered_seqs, alphabet=alignment._alphabet)\n",
    "\n",
    "    return new_alignment\n",
    "\n",
    "# Apply gaps removing to all files\n",
    "def process_folder(input_folder):\n",
    "    # Iterate over all input files in the input folder\n",
    "    for input_file in os.listdir(input_folder):\n",
    "        if input_file.endswith('.fasta'):\n",
    "            # Set file paths\n",
    "            file_path = os.path.join(input_folder, input_file)\n",
    "\n",
    "            # Read the alignment and remove extra gaps\n",
    "            alignment = AlignIO.read(file_path, 'fasta')\n",
    "            filtered_alignment = remove_extra_gaps(alignment)\n",
    "\n",
    "            # Write the filtered alignment to a new file\n",
    "            with open(file_path, \"w\") as output_handle:\n",
    "                AlignIO.write(filtered_alignment, output_handle, 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove gaps from all ClustalOmega MSA\n",
    "folder = f'{directory}/results/alignments/output_files/clustal'\n",
    "\n",
    "process_folder(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the following parameters for MSA ClustalOmega:\n",
    "\n",
    "- Output format: Pearson/FASTA\n",
    "- Order: input\n",
    "\n",
    "Next, we will save the outputs in the corresponding directories. After that, we need to open the files in Jalview and remove gaps in the first line corresponding to the query Uniprot ID to maintain the correct length of sequences.\n",
    "As a result, we have 2 alignments for comparison: BLAST and ClustalOmega."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Preparing the proteins for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T09:29:48.997839Z",
     "start_time": "2023-09-14T09:29:48.889688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'blast_seqs' (ndarray)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('P00392', (200, 561))"
      ]
     },
     "execution_count": 891,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare all sequences for the following comparison - the alignments fasta files\n",
    "blast_path = f'{directory}/results/alignments/output_files/blast/'\n",
    "# al_blast = f'{directory}/results/alignments/output_files/blast/{id_split}_blast.fasta'\n",
    "# al_clustal = f'{directory}/results/alignments/output_files/clustal/{id_split}_clustal.fasta'\n",
    "\n",
    "# Make a dataframes of these alignments\n",
    "# blast_seqs = get_seqs(al_blast)\n",
    "blast_seqs = print_dis_seqs(blast_path, 'blast', id_split)\n",
    "# clustal_seqs = get_seqs(al_clustal)\n",
    "%store blast_seqs \n",
    "# clustal_seqs\n",
    "\n",
    "# The number of rows and columns (sequences and length of the sequence)\n",
    "id_split, blast_seqs.shape # clustal_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "metadata": {},
   "outputs": [],
   "source": [
    "# blast_seqs_dict = {}\n",
    "\n",
    "# for ind_q, row_q in disprot_sequences.iterrows():\n",
    "#     query_id = row_q['disprot_id']\n",
    "    \n",
    "#     for blast_file in os.listdir(blast_path):\n",
    "#         blast_seqs = print_dis_seqs(blast_path, 'blast', query_id)\n",
    "#         blast_seqs_dict[blast_file] = blast_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for blast_file, blast_seqs in blast_seqs_dict.items():\n",
    "#     print(f\"BLAST file: {blast_file}, MSA shape: {blast_seqs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the statistics\n",
    "# Returns the values of occupancy and entropy for each alignment\n",
    "def stats_calculation(seqs, q_id):\n",
    "    \n",
    "    data = []\n",
    "    aa = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "\n",
    "    for i, column in enumerate(seqs.T):\n",
    "\n",
    "        count = Counter(column)\n",
    "        try:\n",
    "            count.pop('-')\n",
    "        except KeyError:\n",
    "            pass\n",
    "        count_sorted = sorted(count.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        non_gap = np.count_nonzero(column != '-')\n",
    "        occupancy = non_gap / column.size\n",
    "\n",
    "        probabilities = [count.get(k, 0.0) / column.size for k in aa]\n",
    "\n",
    "        entropy = scipy.stats.entropy(probabilities, base=20)\n",
    "        data.append([i, q_id, occupancy, entropy, count_sorted])\n",
    "\n",
    "    df_calc = pd.DataFrame(data, columns=['pos', 'query_id', 'occupancy', 'entropy', 'counts'])\n",
    "    return df_calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [],
   "source": [
    "blast_calc = stats_calculation(blast_seqs, id_split) # initial MSA BLAST\n",
    "blast_calc.to_csv(f'results/stats/blast_calc_{id_split}.csv')\n",
    "# clustal_calc = stats_calculation(clustal_seqs, id_split) # initial MSA ClustalOmega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the stats_total dataframe: 21870\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>query_id</th>\n",
       "      <th>occupancy</th>\n",
       "      <th>entropy</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>P01019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[('M', 200)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>P01019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.346925</td>\n",
       "      <td>[('A', 143), ('T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>P01019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.110752</td>\n",
       "      <td>[('P', 186), ('A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>P01019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.191072</td>\n",
       "      <td>[('A', 169), ('T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>P01019</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.211104</td>\n",
       "      <td>[('G', 166), ('S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pos query_id  occupancy   entropy               counts\n",
       "0    0   P01019        1.0  0.000000         [('M', 200)]\n",
       "1    1   P01019        1.0  0.346925  [('A', 143), ('T...\n",
       "2    2   P01019        1.0  0.110752  [('P', 186), ('A...\n",
       "3    3   P01019        1.0  0.191072  [('A', 169), ('T...\n",
       "4    4   P01019        1.0  0.211104  [('G', 166), ('S..."
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge files with statistics\n",
    "folder_path = 'results/stats/'\n",
    "stats_total = pd.DataFrame()\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        data = pd.read_csv(file_path, index_col=0)\n",
    "        stats_total = pd.concat([stats_total, data], ignore_index=True)\n",
    "\n",
    "stats_total.to_csv('results/stats_total.csv')\n",
    "print(f'The length of the stats_total dataframe: {len(stats_total)}')\n",
    "stats_total.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n",
      "['P01019' 'A1L1Q4' 'Q9BYI3' 'Q9H832' 'Q5T4W7' 'S7W634' 'Q8IW19' 'Q93KQ4'\n",
      " 'Q9CXY6' 'Q8IU57' 'J8TM36' 'O43474' 'Q84852' 'O88339' 'Q8R464' 'Q99967'\n",
      " 'A8AZZ3' 'P07342' 'P01097' 'Q9Z2F5' 'Q8K4J6' 'O00585' 'P00392' 'O14727'\n",
      " 'Q86FP8' 'P00742' 'P04370-5' 'Q5VZK9' 'Q8WUG5' 'O35274' 'S6B291' 'P00736'\n",
      " 'A4L7I2' 'O15922' 'O00308' 'Q9BYF1' 'O14958' 'O43791' 'Q9H0E2']\n"
     ]
    }
   ],
   "source": [
    "print(stats_total['query_id'].nunique())\n",
    "print(stats_total['query_id'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Calculating and removing redundant regions from MSA\n",
    "We will use CD-Hit tool. Given the aligned sequences as an input and setting the threshold of 62% we will keep only non-redundant regions in MSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate redundancy \n",
    "# def calculate_Nf(msa_file, threshold, id_split):\n",
    "\n",
    "#     output_file = f\"/Users/alina/HMM/results/alignments/input_files/non-redundant/Nf_{id_split}.fasta\"\n",
    "#     cd_hit_path = \"/Users/alina/cd-hit/cd-hit\"\n",
    "\n",
    "#     # Run CD-HIT to cluster the sequences (excluding the first line) and remove redundancy\n",
    "#     cmd = f\"{cd_hit_path} -i {msa_file} -o {output_file} -c {threshold} -n 4 > /dev/null\"\n",
    "#     subprocess.call(cmd, shell=True)\n",
    "\n",
    "#     # Read the first line from the original MSA file\n",
    "#     with open(msa_file, \"r\") as msa_handle:\n",
    "#         first_record = next(SeqIO.parse(msa_handle, \"fasta\"))\n",
    "\n",
    "#     # Temporarily store the non-redundant sequences in a list\n",
    "#     non_redundant_sequences = []\n",
    "#     with open(output_file, \"r\") as output_handle:\n",
    "#         for record in SeqIO.parse(output_handle, \"fasta\"):\n",
    "#             non_redundant_sequences.append(record)\n",
    "\n",
    "#     # Write the non-redundant sequences to the output file\n",
    "#     with open(output_file, \"w\") as final_handle:\n",
    "#         SeqIO.write([first_record] + non_redundant_sequences, final_handle, \"fasta\")\n",
    "\n",
    "#     # Count the number of sequences in the MSA and the non-redundant MSA\n",
    "#     total_sequences = sum(1 for record in SeqIO.parse(msa_file, \"fasta\"))\n",
    "#     non_redundant_sequences_count = len(non_redundant_sequences)\n",
    "\n",
    "#     # Calculate the effective sequences (Nf)\n",
    "#     Nf = non_redundant_sequences_count / total_sequences\n",
    "#     print(\"The number of non-redundant sequences:\", non_redundant_sequences_count)\n",
    "#     print(\"The total number of sequences:\", total_sequences)\n",
    "#     print(\"The ratio of non-redundant sequences (Nf):\", \"{:.2f}\".format(Nf))\n",
    "\n",
    "#     return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T09:31:24.093075Z",
     "start_time": "2023-09-14T09:31:23.979602Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate redundancy and the number of effective sequences with calculate_Nf function using the .fasta as an input\n",
    "# ali_file = f'{directory}/results/alignments/input_files/{id_split}_input_1.fasta'\n",
    "# print(id_split)\n",
    "# calculate_Nf(ali_file, 0.62, id_split) \n",
    "\n",
    "# calculate_Nf(al_blast, 0.62, id_split)\n",
    "# calculate_Nf(al_clustal, 0.62, id_split)\n",
    "# calculate_Nf(al_mafft, 0.62, id_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T09:31:29.005499Z",
     "start_time": "2023-09-14T09:31:28.994168Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Save the generated cluster file for the MSA notebook\n",
    "# data_file = f\"/Users/alina/HMM/results/alignments/input_files/non-redundant/Nf_{id_split}.fasta.clstr\"\n",
    "# %store data_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T09:31:30.247223Z",
     "start_time": "2023-09-14T09:31:30.210308Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Load non_redundant MSA from ClustalOmega\n",
    "# non_redundant = f'{directory}/results/alignments/output_files/clustal/non-redundant/Nf_{id_split}_clustal.fasta'\n",
    "\n",
    "# nr_seqs = get_seqs(non_redundant)\n",
    "# %store nr_seqs\n",
    "\n",
    "# # Check the shape of non-redundant MSA\n",
    "# print(f\"The shape of non-redundant MSA for {id_split} protein:\", nr_seqs.shape)\n",
    "# nr_seqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-02T13:49:05.791602Z",
     "start_time": "2023-08-02T13:49:05.786460Z"
    }
   },
   "source": [
    "### 1.5 Defining disordered regions in MSA\n",
    "\n",
    "Here we will look at the positions of disordered regions in an alignment. Then we'll extract these regions for the separate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T09:31:32.560245Z",
     "start_time": "2023-09-14T09:31:32.539609Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Extract the lists of start and end regions\n",
    "# start_regions = curated_query['start'].tolist()\n",
    "# end_regions = curated_query['end'].tolist()\n",
    "# print(f'The lists of start and end positions of the {id_split} disordered regions: \\n'\n",
    "#       f'Start regions: {start_regions}, \\n'\n",
    "#       f'End regions: {end_regions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_directory = f'{directory}/results/alignments/output_files/disordered'\n",
    "\n",
    "# start_regions = []\n",
    "# end_regions = []\n",
    "\n",
    "# for i, row in curated_disprot.iterrows():\n",
    "# #     id_dis = row.loc['acc']\n",
    "#     start_regions.append(row.loc['start'])\n",
    "#     end_regions.append(row.loc['end'])\n",
    "#     separate_disordered_regions = select_dis_regions(al_blast, id_dis, start_regions, end_regions, output_directory)\n",
    "\n",
    "# print(f\"The lists of start and end positions of the disordered regions: \\n\"\n",
    "#           f\"Start regions: {start_regions}, \\n\"\n",
    "#           f\"End regions: {end_regions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1 Define disordered regions for the redundant MSAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves disordered regions from MSA output in separate FASTA files\n",
    "def select_dis_regions(msa_file, query_id, output_directory):\n",
    "\n",
    "    sep_sequences = []  # Collect trimmed sequences\n",
    "    id_split = query_id.split('_')[0]\n",
    "    start_position = int(query_id.split('_')[1].split('-')[0])\n",
    "    end_position = int(query_id.split('_')[1].split('-')[1])\n",
    "    print(id_split, start_position, end_position)\n",
    "\n",
    "    with open(msa_file, 'r') as msa_handle:\n",
    "        msa_records = list(SeqIO.parse(msa_handle, 'fasta'))\n",
    "\n",
    "    records = []\n",
    "    query_record_id = str(query_id)\n",
    "    subject_record_ids = []\n",
    "\n",
    "    for j, record in enumerate(msa_records):\n",
    "        sequence = record.seq\n",
    "        if len(sequence) >= start_position > 0 and end_position <= len(sequence):\n",
    "            trimmed_sequence = sequence[start_position - 1: end_position]\n",
    "\n",
    "            if j == 0:\n",
    "                record_id = query_record_id\n",
    "                description = ''\n",
    "            else:\n",
    "                if record.id not in subject_record_ids:\n",
    "                    subject_record_ids.append(record.id)\n",
    "                record_id = subject_record_ids[-1]\n",
    "                description = record.description\n",
    "\n",
    "            disordered_record = SeqIO.SeqRecord(trimmed_sequence, id=record_id, description=description)\n",
    "            records.append(disordered_record)\n",
    "        else:\n",
    "            print(f'Invalid region: start={start_position}, end={end_position}')\n",
    "\n",
    "    if records:\n",
    "        output_file_separate = os.path.join(output_directory, f'{query_id}.fasta')\n",
    "        SeqIO.write(records, output_file_separate, 'fasta')\n",
    "\n",
    "    sep_sequences.extend([record.seq for record in records])  # Extend the collected sequences\n",
    "\n",
    "    return sep_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dis_seqs(directory, align_type, query_id):\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        if f'{query_id}_' in file_name and f'_disordered' not in file_name:\n",
    "            alignment_file = os.path.join(directory, file_name)\n",
    "            seqs = []\n",
    "\n",
    "            with open(alignment_file) as f:\n",
    "                for record in AlignIO.read(f, 'fasta'):\n",
    "                    seqs.append(np.array(list(record.seq), dtype='str'))\n",
    "\n",
    "    if seqs:\n",
    "        return np.array(seqs, dtype='str')\n",
    "    else:\n",
    "        return np.array([])  # Return an empty array if no sequences were found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-14T09:31:35.773114Z",
     "start_time": "2023-09-14T09:31:35.728344Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q8IW19 450 511\n",
      "Stored 'dis_seqs' (ndarray)\n",
      "Q8IW19 (200, 62) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Split the disordered regions with the select_dis_regions function\n",
    "output_directory = f'{directory}/results/alignments/output_files/disordered'\n",
    "separate_disordered_regions = select_dis_regions(al_blast, id_dis, output_directory)\n",
    "\n",
    "# if there is more than 1 region\n",
    "dis_seqs = print_dis_seqs(output_directory, 'disordered', id_split)\n",
    "%store dis_seqs\n",
    "# if isinstance(dis_seqs, list):  # Check if dis_seqs is a list of several disordered regions\n",
    "#     print(id_split, dis_seqs[0].shape, type(dis_seqs[0]))\n",
    "#     print(id_split, dis_seqs[1].shape, type(dis_seqs[1]))\n",
    "# else: # if dis_seqs consists of one disordered region only\n",
    "#     print(id_split, dis_seqs.shape, type(dis_seqs))\n",
    "    \n",
    "print(id_split, dis_seqs.shape, type(dis_seqs))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 62)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dis_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 651)"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blast_seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T07:09:42.452981Z",
     "start_time": "2023-08-21T07:09:42.440758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'dis_calc' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# # Calculation of occupancy and entropy\n",
    "# if isinstance(dis_seqs, list):\n",
    "#     dis_calc0 = stats_calculation(dis_seqs[0], id_split)\n",
    "#     dis_calc1 = stats_calculation(dis_seqs[0], id_split)\n",
    "#     %store dis_calc0 dis_calc1\n",
    "# else:\n",
    "dis_calc = stats_calculation(dis_seqs, id_split)\n",
    "%store dis_calc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.2 Define disordered regions for the non-redundant MSAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-21T07:09:47.036332Z",
     "start_time": "2023-08-21T07:09:47.019014Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q8IW19 450 511\n",
      "Stored 'dis_seqs_nr' (ndarray)\n",
      "Q8IW19 (4, 62) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Split the disordered regions with the help of select_dis_regions function\n",
    "output_directory = f\"{directory}/results/alignments/output_files/disordered/non-redundant\"\n",
    "separate_disordered_regions = select_dis_regions(non_redundant, id_dis, output_directory)\n",
    "\n",
    "# if there are more than 1 region\n",
    "dis_seqs_nr = print_dis_seqs(output_directory, 'disordered', id_split)\n",
    "%store dis_seqs_nr\n",
    "# if isinstance(dis_seqs, list):  # Check if dis_seqs is a list of several disordered regions\n",
    "#     print(id_dis, dis_seqs_nr[0].shape, type(dis_seqs_nr[0]))\n",
    "#     print(id_dis, dis_seqs_nr[1].shape, type(dis_seqs_nr[1]))\n",
    "# else: # if dis_seqs consists of one disordered region only\n",
    "print(id_split, dis_seqs_nr.shape, type(dis_seqs_nr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-07T12:47:10.667068Z",
     "start_time": "2023-08-07T12:47:10.651948Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'dis_calc_nr' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# # Calculation of occupancy and entropy\n",
    "# if isinstance(dis_seqs, list):\n",
    "#     dis_calc_nr0 = stats_calculation(dis_seqs_nr[0], id_split)\n",
    "#     dis_calc_nr1 = stats_calculation(dis_seqs_nr[0], id_split)\n",
    "#     %store dis_calc_nr0 dis_calc_nr1\n",
    "# else:\n",
    "dis_calc_nr = stats_calculation(dis_seqs_nr, id_split)\n",
    "%store dis_calc_nr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
